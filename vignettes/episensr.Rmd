---
title: "Quantitative Bias Analysis for Epidemiologic Data"
author: "Denis Haine"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Quantitative Bias Analysis for Epidemiologic Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Quantitative bias analysis allows to estimate nonrandom errors in epidemiologic
studies, assessing the magnitude and direction of biases, and quantifying their
uncertainties.
Every study has some random error due to its limited sample size, and is
susceptible to systematic errors as well, from selection bias to the presence of
(un)known confounders or information bias (measurement error, including
misclassification).
Bias analysis methods were compiled by Lash et al. in their
book
["Applying Quantitative Bias Analysis to Epidemiologic Data"](https://www.springer.com/us/book/9780387879604).
This package implements the various bias analyses from that book, which are also
[available](https://sites.google.com/site/biasanalysis/) (for some) as a
spreadsheet or a SAS macro.
This vignette provides some examples on how to use the package.

(_Note: A Shiny app is available for some functions, at <https://dhaine.shinyapps.io/episensr_shiny/>._)

## Selection Bias

We will use a case-control study by
[Stang et al.](http://www.ncbi.nlm.nih.gov/pubmed/16523014) on the relation
between mobile phone use and uveal melanoma.
The observed odds ratio for the association between regular mobile phone use vs.
no mobile phone use with uveal melanoma incidence is 0.71 [95% CI 0.51-0.97].
But there was a substantial difference in participation rates between cases and
controls (94% vs 55%, respectively) and so selection bias could have an impact
on the association estimate.
The 2X2 table for this study is the following:

|          | Regular use | No use |
|---------:|:-----------:|:------:|
| Cases    | 136         | 107    |
| Controls | 297         | 165    |

We use the function `selection` as shown below.

```{r selection}
library(episensr)

stang <- selection(matrix(c(136, 107, 297, 165),
                          dimnames = list(c("UM+", "UM-"), c("Mobile+", "Mobile-")),
                          nrow = 2, byrow = TRUE),
                   bias_parms = c(.94, .85, .64, .25))
stang
```

The various `episensr` functions return an object which is a list containing the
input and output variables.
You can check it out with `str()`.

The 2X2 table is provided as a matrix and selection probabilities given with the
argument `bias_parms`, a vector with the 4 probabilities (guided by the
participation rates in cases and controls) in the following order: among cases
exposed, among cases unexposed, among noncases exposed, and among noncases
unexposed.
The output shows the observed 2X2 table and the observed odds ratio (and relative
risk), followed by the corrected ones.

## Uncontrolled Confounders

We will use data from a cross-sectional study by
[Tyndall et al.](http://www.ncbi.nlm.nih.gov/pubmed/8879763) on the association
between male circumcision and the risk of acquiring HIV, which might be
confounded by religion.
The code to account for unmeasured or unknown confounders is the following,
where the 2X2 table is given as a matrix.
We choose a risk ratio implementation, provide a vector defining the risk ratio
associating the confounder with the disease, and the prevalence of the
confounder, religion, among the exposed and the unexposed.

```{r confounders}
confounders(matrix(c(105, 85, 527, 93),
                   dimnames = list(c("HIV+", "HIV-"), c("Circ+", "Circ-")),
                   nrow = 2, byrow = TRUE),
            type = "RR",
            bias_parms = c(.63, .8, .05))
```

The output gives the crude 2X2 table, the crude relative risk and confounder specific measures of
association between exposure and outcome, and the relationship adjusted for the
unknown confounder, using a standardized morbidity ratio (SMR) or a
Mantel-Haenszel (MH) estimate of the risk ratio.

## Unmeasured confounding and E-value

E-value, introduced by [VanderWeele et
al.](https://annals.org/aim/article-abstract/2643434/sensitivity-analysis-observational-research-introducing-e-value),
is the minimum strength of association on the risk ratio scale that an
unmeasured confounder would need to have with both the exposure and the outcome,
conditional on the measured covariates, to fully explain away a specific
exposure-outcome association.

For example, a study by [Victoria et
al.](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(87)90902-0/fulltext)
found an association between formula-fed babies compared to breast-fed and
respiratory death, RR = 3.9. What would be the strength of association for an
unmeasured confounder needed to explain away this association? The E-value
express the magnitude of the confounder associations that can produce
confounding bias equal to the observed exposure-outcome association. This E-value is:

```{r e-value-1}
confounders.evalue(est = 3.9, lower_ci = 1.8, upper_ci = 8.7, type = "RR")
```

The E-value is 7.26, i.e. an unmeasured confounder must have a risk ratio of at
least 7.26 with both the exposure and the outcome, and above and beyond the
measured confounders, to explain away the observed risk ratio of 3.9. A weaker
unmeasured confounder could not explain this RR. E-value for the CI closest to
the null (i.e. 1) can also be reported.

Similarly, the minimum magnitude of both confounder associations to move away
the observed association can be assessed for a non-null hypothesis. For example, the
effect of anti-depressant use during pregnancy on infant cardiac defects was
1.06 [0.93--1.22] ([Huybrechts et
al.](https://www.nejm.org/doi/full/10.1056/NEJMoa1312828)). The confidence
interval includes 1 so no further unmeasured confounding is needed to shift the
confidence interval to include 1, i.e. the E-value = 1. But what would be the
strength of a confounder needed to shift RR to 1.2?

```{r e-value-2}
confounders.evalue(est = 1.06, lower_ci = 0.93, upper_ci = 1.22,
                   type = "RR", true_est = 1.2)
```

We need an unmeasured confounder with a RR of 1.5 each with the outcome, cardiac
defect, and the exposure, anti-depressant use, to move the risk ratio to 1.2. A
weaker confounder could not shift the observed RR.

E-value can also be computed when the effect measure is:

* OR or HR for rare outcomes (i.e. < 15%), rate ratio for count and continuous
  outcomes: `RR`,
* OR for common outcomes (i.e. > 15%): `ORc`,
* HR for common outcomes (i.e. > 15%): `HRc`,
* Difference in continuous outcomes, with standardized effect size *d* (mean of
  the outcome variable divided by the SD of the outcome) and an SE for this
  standardized effect size *s_d*, either with RR approximation or OR
  approximation (meta-analysis): `diff_RR` and `diff_OR`.

## Probabilistic Sensitivity Analysis for Exposure Misclassification

We use a study on the effect of smoking during pregnancy on breast cancer risk
by [Fink & Lash](http://www.ncbi.nlm.nih.gov/pubmed/12946045), where we assume
nondifferential misclassification of the exposure, smoking, with probability
density functions for sensitivities (Se) and specificities (Sp) among cases and
noncases equal to uniform distributions with a minimum of 0.7 and a maximum of
0.95 for sensitivities (0.9 and 0.99 respectively for specificities).
We choose to correct for exposure misclassification with the argument `type =
exposure`.
We ask for 50000 replications (default is 1000).
Don't be shy with the number of iterations, episensr is fast.

The Se and Sp for cases (`seca`, `spca`) are given as a list with its first
element referring to the type of distribution (choice between constant, uniform,
triangular, trapezoidal, logit-logistic, and logit-normal) and the second
element giving the distribution parameters (min and max for uniform
distribution).
By avoiding to provide information on the noncases (`seexp`, `spexp`), we are
referring to a nondifferential misclassification.

```{r probsens}
set.seed(123)
smoke.nd <- probsens(matrix(c(215, 1449, 668, 4296),
                            dimnames = list(c("BC+", "BC-"), c("Smoke+", "Smoke-")),
                            nrow = 2, byrow = TRUE),
                     type = "exposure",
                     reps = 50000,
                     seca.parms = list("uniform", c(.7, .95)),
                     spca.parms = list("uniform", c(.9, .99)))
smoke.nd
```

The output gives the 2X2 table, the observed measures of association, and the
corrected measures of association.

We saved the `probsens` analysis in a new object `smoke.nd`.
We can see its elements with the function `str()`:

```{r str}
str(smoke.nd)
```

`smoke.nd` is a list of 4 elements where different information on the analysis
done are saved.
We have `smoke.nd$obs.data` where we have the observed 2X2 table,
`smoke.nd$obs.measures` (the observed measures of association),
`smoke.nd$adj.measures` (the adjusted measures of association), and
`smoke.nd$sim.df`, a data frame with the simulated variables from each
replication, like the Se and Sp, the 4 cells of the adjusted 2X2 table, and the
adjusted measures.
We can plot the Se prior distribution (and not forgetting to discard the draws that
led to negative adjustments).

```{r plot, fig.cap = "Sensibility prior distribution."}
hist(smoke.nd$sim.df[!is.na(smoke.nd$sim.df$corr.RR), ]$seca,
     breaks = seq(0.65, 1, 0.01),
     col = "lightgreen",
     main = NULL,
     xlab = "Sensitivity for Cases")
```

There are combinations of Se, Sp, and disease (or exposure) prevalence that
  produce negative cells in the corrected 2-by-2 table.
  For outcome   misclassification, this happen when the frequency of observed
  exposed cases is less than the total number of diseased individuals multiplied
  by the false-positive proportion.
  Negative cell counts occur when the false-positive proportion is greater than
  the proportion of cases that are exposed.
  When providing values for Se and Sp that are more or less like random
  classification (i.e. Se ~50% and Sp ~50%), you obtain negative cell values.

## Probabilistic Sensitivity Analysis for Uncontrolled Confounding

Let's illustrate this function with this example from [Modern Epidemiology by
Rothman, Greenland &
Lash](https://shop.lww.com/Modern-Epidemiology/p/9781451190052), where the
association between occupational resins exposure and lung cancer mortality is
studied, together with the presence of an unmeasured potential confounder,
smoking ([Greenland et al., 1994](https://www.ncbi.nlm.nih.gov/pubmed/7927843)).

|          | Resins exposed | Resins unexposed |
|---------:|:--------------:|:----------------:|
|    Cases | 45             | 94               |
| Controls | 257            | 945              |

Prior probability distributions are given to each bias parameters.
Prevalences of smoking in those exposed to resins, and of smoking in those
unexposed to resins receive prior distributions that are uniform between 0.40
and 0.70. 
Prior distribution for the odds ratio associating smoking with lung cancer
mortality is log-normal with 95% limits of 5 and 15.
The mean of this distribution is [ln(15) + ln(5)] / 2 = 2.159, and its standard
deviation is [ln(15) - ln(5)] / 3.92 = 0.28.

```{r probsens-conf}
set.seed(123)
probsens.conf(matrix(c(45, 94, 257, 945),
                     dimnames = list(c("Cases+", "Cases-"), c("Res+", "Res-")),
                     nrow = 2, byrow = TRUE),
              reps = 50000,
              prev.exp = list("uniform", c(.4, .7)),
              prev.nexp = list("uniform", c(.4, .7)),
              risk = list("log-normal", c(2.159, .28)))
```

The median adjusted OR is 1.76 [1.25,2.49].

## Bootstrapping

Selection and misclassification bias models can be bootstrapped in order to get
confidence interval on the adjusted association parameter.
We can use the ICU dataset from Hosmer and
Lemeshow
[Applied Logistic Regression](http://ca.wiley.com/WileyCDA/WileyTitle/productCd-0470582472.html) textbook
as an example.
The replicates that give negative cells in the adjusted 2X2 table are silently
ignored and the number of effective bootstrap replicates is provided in the
output.
Ten thousands bootstrap samples are a good number for testing everything runs
smoothly.
But again, don't be afraid of running more, like 100,000 bootstrap samples.

```{r, boot}
library(aplore3)  # to get ICU data
data(icu)

## First run the model
misclass_eval <- misclassification(icu$sta, icu$inf,
                                   type = "exposure",
                                   bias_parms = c(.75, .85, .9, .95))
misclass_eval

## Then bootstrap it
set.seed(123)
misclass_boot <- boot.bias(misclass_eval, R = 10000)
misclass_boot
```

Bootstrap replicates can also be plotted, with the confidence interval shown as
dotted lines.

```{r, boot_fig, fig.cap = "Bootstrap replicates and confidence interval.", warning=F}
plot(misclass_boot, "rr")
```

